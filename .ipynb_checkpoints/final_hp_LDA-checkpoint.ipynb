{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "all\n",
      "am\n",
      "an\n",
      "and\n",
      "any\n",
      "are\n",
      "as\n",
      "at\n",
      "be\n",
      "because\n",
      "been\n",
      "before\n",
      "being\n",
      "below\n",
      "between\n",
      "both\n",
      "but\n",
      "by\n",
      "cannot\n",
      "could\n",
      "did\n",
      "do\n",
      "does\n",
      "doing\n",
      "down\n",
      "during\n",
      "each\n",
      "few\n",
      "for\n",
      "from\n",
      "further\n",
      "had\n",
      "has\n",
      "have\n",
      "having\n",
      "he\n",
      "her\n",
      "here\n",
      "hers\n",
      "herself\n",
      "him\n",
      "himself\n",
      "his\n",
      "how\n",
      "i\n",
      "if\n",
      "in\n",
      "into\n",
      "is\n",
      "it\n",
      "its\n",
      "itself\n",
      "me\n",
      "more\n",
      "most\n",
      "my\n",
      "myself\n",
      "no\n",
      "nor\n",
      "not\n",
      "of\n",
      "off\n",
      "on\n",
      "once\n",
      "only\n",
      "or\n",
      "other\n",
      "ought\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "out\n",
      "over\n",
      "own\n",
      "same\n",
      "she\n",
      "should\n",
      "shouldn't\n",
      "so\n",
      "some\n",
      "such\n",
      "than\n",
      "that\n",
      "the\n",
      "their\n",
      "theirs\n",
      "them\n",
      "themselves\n",
      "then\n",
      "there\n",
      "these\n",
      "they\n",
      "this\n",
      "those\n",
      "through\n",
      "to\n",
      "too\n",
      "under\n",
      "until\n",
      "up\n",
      "very\n",
      "was\n",
      "wasn't\n",
      "we\n",
      "were\n",
      "what\n",
      "when\n",
      "where\n",
      "which\n",
      "while\n",
      "who\n",
      "whom\n",
      "why\n",
      "with\n",
      "would\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "chapter\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sub() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7790fe2212d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'[^a-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# if it's only one or two letter, delete it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'\\s[a-z]{2}\\s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange as trange\n",
    "\n",
    "#q = topic for each word\n",
    "#pi = mixed topic distribution for document\n",
    "#y = words\n",
    "#b = word distribution per topic\n",
    "\n",
    "vocab = set()\n",
    "docs = []\n",
    " \n",
    "D = 36 # number of chapters\n",
    "K = 15 # number of topics\n",
    "stops = open('hp_deathlyhallows/stopwords.txt').read()\n",
    "print stops\n",
    "\n",
    "\n",
    "# open each file; convert everything to lowercase and strip non-letter symbols; split into words\n",
    "for fileind in range( 1, D+1 ):\n",
    "    foo = open( 'hp_deathlyhallows/chapter%02d.txt' % fileind ).read()    \n",
    "    # if it's capital, lower case it. If it's not a char, delete it.\n",
    "    tmp = re.sub( '[^a-z]+', ' ', foo.lower() ).split()\n",
    "    # if it's only one or two letter, delete it\n",
    "    temp = re.sub( '\\s[a-z]{2}\\s', '', tmp)\n",
    "    docs.append( temp )\n",
    "    for w in tmp:\n",
    "        for stop in stops:\n",
    "            if (w != stop):\n",
    "                 vocab.add( w )\n",
    "\n",
    "\n",
    "# vocab now has unique words\n",
    "# give each word in the vocab a unique id\n",
    "ind = 0\n",
    "vhash = {}\n",
    "vindhash = {}\n",
    "for i in list(vocab):\n",
    "    vhash[i] = ind\n",
    "    vindhash[ind] = i\n",
    "    ind += 1\n",
    "\n",
    "# size of our vocabulary\n",
    "V = ind\n",
    "\n",
    "# reprocess each document and re-represent it as a list of word ids\n",
    "docs_i = []\n",
    "for d in docs:\n",
    "    dinds = []\n",
    "    for w in d:\n",
    "        dinds.append(vhash[w])\n",
    "    docs_i.append(dinds)\n",
    "\n",
    "\n",
    "# compute the likelihood by using \n",
    "def compute_data_likelihood(docs_i, qs, topics, pdtm):\n",
    "        # sum of log likelihood\n",
    "        ssum = 0\n",
    "        for d,doc in enumerate(docs_i):\n",
    "            for w,word in enumerate(doc):\n",
    "                # k = topic of the word in that document\n",
    "                k = qs[d][w]\n",
    "                #add the probability of that word in that doc\n",
    "                ssum += np.log(topics[k][word])\n",
    "        return ssum\n",
    "            \n",
    "def randomly_assign_topics(docs_i, K):\n",
    "    qs = []\n",
    "    for doc in docs_i:\n",
    "        qs.append(np.random.randint(0, high=K, size=len(doc)))\n",
    "    return qs\n",
    "\n",
    "def pdtm_resample(pdtm, alphas, cik):\n",
    "    for d in xrange(D):\n",
    "            pdtm[d] = np.random.dirichlet(alphas+cik[d])\n",
    "    return pdtm\n",
    "\n",
    "def topics_resample( topics, gammas, cvk):\n",
    "    for k in xrange(K):\n",
    "        topics[k] = np.random.dirichlet(gammas+cvk[:,k])\n",
    "    return topics\n",
    "\n",
    "#update civk and resample qs\n",
    "def qs_resample(docs_i, qs, topics, pdtm, collapsed, cik, cvk, Li, gammas, alphas):\n",
    "    for d,doc in enumerate(docs_i):\n",
    "        for w,word in enumerate(doc):\n",
    "            kbyword = np.zeros(K)\n",
    "            for k in xrange(K):\n",
    "                if (collapsed):\n",
    "                    first = (cvk[word,k] + 1)/(ck[k]+V)\n",
    "                    second = (cik[d,k] + 1)/(Li[d]+K)\n",
    "                    kbyword[k] = first*second\n",
    "                else:\n",
    "                    kbyword[k] = pdtm[d][k]*topics[k][word]\n",
    "            # norm kbyword\n",
    "            kbyword /= np.sum(kbyword)\n",
    "            formerk = qs[d][w]\n",
    "            newk = np.random.choice(K, size = 1, p = kbyword)[0]\n",
    "            \n",
    "            #update all the civk data\n",
    "            qs[d][w] = newk\n",
    "            if(collapsed):\n",
    "                ck[newk] +=1\n",
    "                ck[formerk] -=1\n",
    "            cvk[word,formerk] -=1\n",
    "            cvk[word,newk] +=1\n",
    "            cik[d,formerk] -=1\n",
    "            cik[d, newk] +=1\n",
    "    return qs\n",
    "\n",
    "###########################################\n",
    "qs = randomly_assign_topics( docs_i, K )\n",
    "\n",
    "\n",
    "civk = np.zeros((D,V,K))\n",
    "for d,doc in enumerate(docs_i):\n",
    "    for w,word in enumerate(doc):\n",
    "        k = qs[d][w]\n",
    "        civk[d, word, k] += 1\n",
    "\n",
    "cik = np.sum(civk, axis = 1)\n",
    "cvk = np.sum(civk, axis = 0)\n",
    "ck = np.sum(cik, axis = 0)\n",
    "Li = np.sum(cik, axis = 1)\n",
    "niv = np.sum(civk, axis = 2)\n",
    "\n",
    "\n",
    "alphas = np.ones((K,1))[:,0]\n",
    "gammas = np.ones((V,1))[:,0]\n",
    "\n",
    "# topic distributions initialized as equal for each topic per word\n",
    "topics = np.zeros((K,V))\n",
    "for k in xrange(K):\n",
    "        topics[k] = np.random.dirichlet(gammas+cvk[:,k])\n",
    "\n",
    "#topics= np.random.dirichlet( gammas, K )\n",
    "\n",
    "# per-document-topic distributions initialized as equal for each topic per document\n",
    "pdtm = np.zeros((D, len(alphas)))\n",
    "for d in range(D):\n",
    "    pdtm[d] = np.random.dirichlet(alphas + cik[d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
